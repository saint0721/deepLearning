{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d8704a",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "다차원 배열을 나타내는 자료형. numpy의 ndarray와 유사한 api 제공. GPU 이용한 연산도 지원한다.\n",
    "\n",
    "tensor는 list, numpy를 Pytorch텐서로 변환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf5ec7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = [[1,2,3], [4,5,6]]\n",
    "x = torch.tensor(data, dtype=torch.float32)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05257dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "610bebf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([[1,2,3], [4,5,6]])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "783b6481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.zeros((2, 3, 4))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6010f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1.],\n",
       "          [1., 1.]],\n",
       "\n",
       "         [[1., 1.],\n",
       "          [1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1.],\n",
       "          [1., 1.]],\n",
       "\n",
       "         [[1., 1.],\n",
       "          [1., 1.]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones((2, 2, 2, 2))\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f06e5df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6082, 0.2480, 0.1190],\n",
       "        [0.4936, 0.1134, 0.2126],\n",
       "        [0.9540, 0.4451, 0.3943]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.rand((3, 3))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2acdbc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9670, 0.8581, 0.9777],\n",
       "        [0.3440, 0.7561, 0.4889]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((2, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d3f6dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7180, -0.1126,  1.8470],\n",
       "        [-0.3899, -2.4246, -0.5761]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.randn(2, 3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bacd8784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1, 2, 3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "295055d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.empty(2, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a967c241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.Tensor(2, 3)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf66e5",
   "metadata": {},
   "source": [
    "# from_numpy와 tensor의 차이\n",
    "from_numpy는 numpy 배열을 tensor로 변환한다. 기존 numpy 배열과 메모리를 공유하며, 반환된 tensor를 수정하면 numpy배열도 수정된다.\n",
    "- tensor를 numpy로 변환할 때 numpy로 변환하면 데이터를 공유한다.\n",
    "즉, 동일한 위치를 참조한다.\n",
    "\n",
    "tensor는 numpy와 유사한 tensor를 생성한다. numpy와 메모리를 공유하지 않고, tensor를 수정해도 numpy는 변경되지 않는다.\n",
    "- array로 변환하면 데이터를 공유하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58893cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[-1  2  3]\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "a_np = np.array([1,2,3])\n",
    "print(a_np)\n",
    "a_tensor = torch.from_numpy(a_np)\n",
    "a_tensor[0] = -1\n",
    "print(a_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a8bab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1,  2,  3])\n"
     ]
    }
   ],
   "source": [
    "a_np = np.array([1,2,3])\n",
    "a_tensor = torch.tensor(a_np)\n",
    "a_tensor[0] = -1\n",
    "print(a_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82e5fe",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "데이터 로딩과 전처리를 담당하는 클래스. \n",
    "\n",
    "- len(): 데이터셋의 전체 샘플 개수를 반환하는 메서드. 정수값을 반환한다.\n",
    "- getitem(idx): idx에 해당하는 샘플을 가져오는 메서드. 인덱스에 따라 샘플데이터와 레이블을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "200ea17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, file_path, labels, transform=None):\n",
    "        self.file_path = file_path # 이미지 파일 경로\n",
    "        self.labels = labels # 이미지 레이블 리스트\n",
    "        self.transform = transform # 이미지에 적용한 전처리 함수\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_path)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.file_path[idx])\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57918343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "img_T = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.RandomHorizontalFlip(), # 이미지를 랜덤으로 좌우반전\n",
    "    T.ToTensor(), # Tensor로 변환, 픽셀 값을 [0, 1]범위로 정규화\n",
    "    T.Normalize((0.5), (0.5)) # 각 채널 정규화\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db018790",
   "metadata": {},
   "source": [
    "클래스 정의 후 이미지에 transform적용하는 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67879e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, file_list, label_list, img_T=None):\n",
    "        self.file_list = file_list\n",
    "        self.label_list = label_list \n",
    "        self.img_T = img_T\n",
    "        \n",
    "    def __getitem(self, idx):\n",
    "        image = Image.open(self.file_list[idx])\n",
    "        label = self.label_list[idx]\n",
    "        \n",
    "        if self.img_T is not None:\n",
    "            image = self.img_T(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e74ecb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용하기\n",
    "# dataset = CustomImageDataset(file_list, label_list, img_T=img_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed770ea",
   "metadata": {},
   "source": [
    "# torchvision.transforms\n",
    "함수와 주요 기능\n",
    "- Resize: 이미지의 크기를 조절합니다.\n",
    "- RandomResizedCrop: 이미지를 무작위로 자르고 크기를 조절합니다.\n",
    "- RandomHorizontalFlip: 이미지를 무작위로 수평으로 뒤집습니다.\n",
    "- RandomVerticalFlip: 이미지를 무작위로 수직으로 뒤집습니다.\n",
    "- ToTensor: 이미지를 텐서로 변환합니다.\n",
    "- Normalize: 이미지를 정규화합니다.\n",
    "- ColorJitter: 이미지의 색상을 무작위로 조정합니다.\n",
    "- RandomRotation: 이미지를 무작위로 회전합니다.\n",
    "- RandomCrop: 이미지를 무작위로 자릅니다.\n",
    "- Grayscale: 이미지를 흑백으로 변환합니다.\n",
    "- RandomSizedCrop: 이미지를 무작위로 자르고 크기를 조절합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73884bbc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/SaintKim/Python/Artificial_Intelligence/Deep_Learning/image.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m\n\u001b[1;32m      3\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      4\u001b[0m     T\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)),\n\u001b[1;32m      5\u001b[0m     T\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),\n\u001b[1;32m      6\u001b[0m     T\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      7\u001b[0m     T\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m      8\u001b[0m ])\n\u001b[0;32m----> 9\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m img \u001b[38;5;241m=\u001b[39m preprocess(image)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/PIL/Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/SaintKim/Python/Artificial_Intelligence/Deep_Learning/image.jpg'"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "preprocess = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5), (0.5))\n",
    "])\n",
    "image = Image.open('image.jpg') # 예시로 image.jpg가 들어간것.\n",
    "img = preprocess(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565d668d",
   "metadata": {},
   "source": [
    "# torchvison.utils.save_image\n",
    "딥러닝 모델이 생성한 이미지를 저장할 때 사용\n",
    "\n",
    "`torchvision.utils.save_image(tensor, filename, nrow=8, padding=2, normalize=False, range=None,\n",
    "scale_each=False, pad_value=0`\n",
    "- tensor : (Tensor) - 저장할 이미지 텐서. shape는 (batch_size, channels, height, width) 이어야 합니다.\n",
    "- filename : (str) - 저장할 파일의 경로와 이름입니다.\n",
    "- nrow : (int, optional) - 저장할 이미지들을 한 줄에 몇 개씩 보여줄 것인지 결정하는 인자입니다. 기본값은 8입니다.\n",
    "- padding : (int, optional) - 이미지들 사이의 간격을 몇 개의 픽셀로 할 것인지 결정하는 인자입니다. 기본값은 2입니다.\n",
    "- normalize : (bool, optional) - 이미지의 값을 [0, 1]로 정규화할 것인지 결정하는 인자입니다. 기본값은 True입니다.\n",
    "- range : (tuple, optional) - 이미지를 정규화할 때 사용할 범위를 결정하는 인자입니다. 기본값은 None으로, 입력된 텐서의 값 범위를 그대로 사용합니다.\n",
    "- scale_each : (bool, optional) - 이미지를 정규화할 때 각 이미지마다 다른 범위를 사용할지 여부를 결정하는 인자입니다. 기본값은 False입니다.\n",
    "- pad_value : (float, optional) - 이미지의 테두리를 채우는 값입니다. 기본값은 0입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67cd13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "img = torch.randn(3, 64, 64)\n",
    "vutils.save_image(img, \"img.png\") # RGB의 값으로 무작위로 나오는거 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241cccbb",
   "metadata": {},
   "source": [
    "# DataLoader\n",
    "데이터를 batch단위로 로드해서 효율적인 학습을 가능하게 해주는 클래스\n",
    "\n",
    "인자와 기능\n",
    "\n",
    "- dataset: 데이터를 로드할 데이터셋 객체를 지정합니다. 이는 torch.utils.data.Dataset 클래스를 상속받은 사용자 정의 데이터셋 클래스를 사용하거나, torchvision의 내장 데이터셋 클래스를 사용할 수 있습니다. (기본값: None)\n",
    "\n",
    "- batch_size: 한 번에 로드할 배치(batch)의 크기를 지정합니다. 작은 배치 크기는 더 많은 메모리를 사용하지만 더 자주 모델이 업데이트되고 더 높은 학습 속도를 제공합니다. (기본값: 1)\n",
    "\n",
    "- shuffle: 데이터를 섞을지 여부를 지정합니다. True로 설정할 경우, 데이터가 매 에폭(epoch)마다 섞여서 모델이 각 배치에서 다양한 데이터를 학습하도록 도와줍니다.(기본값: False)\n",
    "\n",
    "- num_workers: 데이터 로딩에 사용할 워커(worker)의 수를 지정합니다. 병렬 처리를 통해 데이터 로딩 속도를 향상시키는 데 사용됩니다.(기본값: 0)\n",
    "\n",
    "- pin_memory: GPU 메모리에 데이터를 고정할지 여부를 지정합니다. GPU를 사용하는 경우 True로 설정하면 데이터가 CPU와 GPU 간에 더 빠르게 복사되어 학습 속도를 향상시킬 수 있습니다. (기본값: False)\n",
    "\n",
    "- collate_fn: 배치를 생성하기 전에 데이터를 결합하는 함수를 지정합니다. 기본값은 None이며, 데이터셋이 출력하는 원시 데이터의 리스트를 배치로 결합합니다. 필요에 따라 사용자 정의 결합 함수를 지정하여 배치를 구성할 수 있습니다.(기본값: None)\n",
    "\n",
    "- drop_last: 마지막 배치의 크기가 batch_size보다 작을 경우, 해당 배치를 무시할지 여부를 지정합니다. True로 설정할 경우 마지막 배치를 무시합니다.(기본값: False)\n",
    "\n",
    "```\n",
    "dataset = CustomImageDataset(file_list, label_list, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "for images, labels in dataloader:\n",
    "    # 학습 수행\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3521b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe889056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0435b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e1592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f8ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc00947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c1047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be946ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ebc393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8505f60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fecd92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592aef18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a65c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917154c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0ed57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6bf50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbbd33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e430f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b9cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c71c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a2619d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8effaa16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d8c0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a911b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf16ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213a299f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197de55f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85c65c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
