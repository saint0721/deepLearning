{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 매개변수 최적화 하기\n",
    "- 모델 학습은 반복적인 과정을 거침.\n",
    "- 출력 추측, 정답과 추측 사이의 loss를 계산하고, 매개변수에 대한 오류의 도함수(derivate)를 수집한 뒤, 경사하강법을 사용해 파라미터를 optimizer한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "\troot = \"data\",\n",
    "\ttrain = True,\n",
    "\tdownload = True,\n",
    "\ttransform = ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "\troot = 'data',\n",
    "\ttrain = False,\n",
    "\tdownload = True,\n",
    "\ttransform = ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.flatten = nn.Flatten()\n",
    "\t\tself.linear_relu_stack = nn.Sequential(\n",
    "\t\t\tnn.Linear(28*28, 512),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(512, 512),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(512, 10)\n",
    "\t\t)\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.flatten(x)\n",
    "\t\tlogits = self.linear_relu_stack(x)\n",
    "\t\treturn logits\n",
    "\n",
    "model = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터(Hyperparameter)\n",
    "모델 최적화 과정을 제어할 수 있는 조절 가능한 매개변수. 서로 다른 하이퍼파라미터 값은 모델학습과 수렴율(convergence rate)에 영향을 미칠 수 있음. \n",
    "- 에폭(Epoch) 수 - 데이터셋을 반복하는 횟수\n",
    "- 배치 크기(Batch_size) - 매개변수가 갱신되기 전 신경망을 통해 전파된 데이터의 샘플 수\n",
    "- 학습률(Learning_rate) - 각 배치/에폭에서 모델의 매개변수를 조절하는 비율. 값이 작을 수록 학습 속도가 느려지고, 값이 크면 학습 중 예측할 수 없는 동작이 발생할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Loop(최적화 단계)\n",
    "하이퍼파라미터를 설정한 뒤에는 모델을 최적화 단계를 통해 학습&최적화가 가능. 각 반복을 epoch이라 부른다.\n",
    "- train loop - 학습용 데이터셋을 반복해서 최적의 매개변수로 수렴\n",
    "- validation/test loop - 모델 성능이 개선되고 있는지를 확인하기 위해 테스트 데이터셋을 반복\n",
    "\n",
    "### Loss function\n",
    "학습용 데이터를 제공하면 학습되지 않은 신경망은 정답이 아닐 확률이 높다. loss function은 획득한 결과와 실제 값 사이의 틀린 정도(degree of dissimilarity)를 측정해 이 값을 최소화한다.\n",
    "- 데이터 샘플을 입력으로 계산하고, 예측과 정답을 비교해서 손실을 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "각 학습단계에서 오류를 줄이기 위해 매개변수를 조절하는 과정. 여기서는 SGD(Stochastic Gradient Descent)\n",
    "- 모든 최적화 절차는 `optimizer`객체에 캡슐화됨. \n",
    "- 학습하려는 모델의 매개변수와 학습률 하이퍼파라미터를 등록해 옵티마이저를 초기화 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loop 최적화 단계는 3개\n",
    "- `optimizer.zero_grad()`를 호출해서 모델 매개변수의 변화도를 재설정한다. 기본적으로 변화도는 add up이기 때문에 중복 계산을 막기 위해 반복마다 0으로 설정함\n",
    "- `loss.backward()`를 호출해서 예측손실(prediction loss)을 역전파한다. \n",
    "- 변화도를 계산한 뒤에는 `optimizer.step()`을 호출해서 역전파 단계에서 수집된 변화도로 매개변수를 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "\tsize = len(dataloader.dataset)\n",
    "\tmodel.train()\n",
    "\n",
    "\tfor batch, (X, y) in enumerate(dataloader):\n",
    "\t\tpred = model(X)\n",
    "\t\tloss = loss_fn(pred, y)\n",
    "\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\tif batch % 100 == 0:\n",
    "\t\t\tloss, current = loss.item(), batch * batch_size + len(X)\n",
    "\t\t\tprint(f\"loss: {loss:>7f} [{current:>5d} / {size::>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "\tsize = len(dataloader.dataset)\n",
    "\tmodel.eval()\n",
    "\tnum_batches = len(dataloader)\n",
    "\ttest_loss, correct = 0, 0\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor X, y in dataloader:\n",
    "\t\t\tpred = model(X)\n",
    "\t\t\ttest_loss = loss_fn(pred, y).item()\n",
    "\t\t\tcorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "\ttest_loss /= num_batches\n",
    "\tcorrect /= size\n",
    "\tprint(f\"Test: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.item()`을 하지 않을 경우 test_loss가 값이아닌 텐서가 저장되서 `test_loss /= num_batches`와 같은 연산에서 경고가 발생할 수 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------\n",
      "loss: 0.447659 [   64 / 60000]\n",
      "loss: 0.563775 [ 6464 / 60000]\n",
      "loss: 0.370152 [12864 / 60000]\n",
      "loss: 0.600224 [19264 / 60000]\n",
      "loss: 0.529290 [25664 / 60000]\n",
      "loss: 0.531633 [32064 / 60000]\n",
      "loss: 0.528099 [38464 / 60000]\n",
      "loss: 0.670642 [44864 / 60000]\n",
      "loss: 0.623863 [51264 / 60000]\n",
      "loss: 0.508385 [57664 / 60000]\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.002020\n",
      "\n",
      "Epoch 2\n",
      "-------------\n",
      "loss: 0.441464 [   64 / 60000]\n",
      "loss: 0.558522 [ 6464 / 60000]\n",
      "loss: 0.366052 [12864 / 60000]\n",
      "loss: 0.594770 [19264 / 60000]\n",
      "loss: 0.524166 [25664 / 60000]\n",
      "loss: 0.527489 [32064 / 60000]\n",
      "loss: 0.523316 [38464 / 60000]\n",
      "loss: 0.671091 [44864 / 60000]\n",
      "loss: 0.622273 [51264 / 60000]\n",
      "loss: 0.501736 [57664 / 60000]\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.002003\n",
      "\n",
      "Epoch 3\n",
      "-------------\n",
      "loss: 0.435487 [   64 / 60000]\n",
      "loss: 0.553607 [ 6464 / 60000]\n",
      "loss: 0.362149 [12864 / 60000]\n",
      "loss: 0.589512 [19264 / 60000]\n",
      "loss: 0.519218 [25664 / 60000]\n",
      "loss: 0.523395 [32064 / 60000]\n",
      "loss: 0.518847 [38464 / 60000]\n",
      "loss: 0.671498 [44864 / 60000]\n",
      "loss: 0.620664 [51264 / 60000]\n",
      "loss: 0.495413 [57664 / 60000]\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.001988\n",
      "\n",
      "Epoch 4\n",
      "-------------\n",
      "loss: 0.429695 [   64 / 60000]\n",
      "loss: 0.548981 [ 6464 / 60000]\n",
      "loss: 0.358449 [12864 / 60000]\n",
      "loss: 0.584391 [19264 / 60000]\n",
      "loss: 0.514422 [25664 / 60000]\n",
      "loss: 0.519363 [32064 / 60000]\n",
      "loss: 0.514680 [38464 / 60000]\n",
      "loss: 0.671847 [44864 / 60000]\n",
      "loss: 0.618981 [51264 / 60000]\n",
      "loss: 0.489382 [57664 / 60000]\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.001972\n",
      "\n",
      "Epoch 5\n",
      "-------------\n",
      "loss: 0.424105 [   64 / 60000]\n",
      "loss: 0.544635 [ 6464 / 60000]\n",
      "loss: 0.354952 [12864 / 60000]\n",
      "loss: 0.579426 [19264 / 60000]\n",
      "loss: 0.509724 [25664 / 60000]\n",
      "loss: 0.515396 [32064 / 60000]\n",
      "loss: 0.510752 [38464 / 60000]\n",
      "loss: 0.672064 [44864 / 60000]\n",
      "loss: 0.617266 [51264 / 60000]\n",
      "loss: 0.483648 [57664 / 60000]\n",
      "Test: \n",
      " Accuracy: 81.6%, Avg loss: 0.001957\n",
      "\n",
      "Epoch 6\n",
      "-------------\n",
      "loss: 0.418661 [   64 / 60000]\n",
      "loss: 0.540512 [ 6464 / 60000]\n",
      "loss: 0.351598 [12864 / 60000]\n",
      "loss: 0.574593 [19264 / 60000]\n",
      "loss: 0.505159 [25664 / 60000]\n",
      "loss: 0.511550 [32064 / 60000]\n",
      "loss: 0.507032 [38464 / 60000]\n",
      "loss: 0.672142 [44864 / 60000]\n",
      "loss: 0.615558 [51264 / 60000]\n",
      "loss: 0.478197 [57664 / 60000]\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.001942\n",
      "\n",
      "Epoch 7\n",
      "-------------\n",
      "loss: 0.413363 [   64 / 60000]\n",
      "loss: 0.536647 [ 6464 / 60000]\n",
      "loss: 0.348443 [12864 / 60000]\n",
      "loss: 0.569951 [19264 / 60000]\n",
      "loss: 0.500741 [25664 / 60000]\n",
      "loss: 0.507775 [32064 / 60000]\n",
      "loss: 0.503489 [38464 / 60000]\n",
      "loss: 0.672073 [44864 / 60000]\n",
      "loss: 0.613806 [51264 / 60000]\n",
      "loss: 0.473012 [57664 / 60000]\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.001927\n",
      "\n",
      "Epoch 8\n",
      "-------------\n",
      "loss: 0.408221 [   64 / 60000]\n",
      "loss: 0.533047 [ 6464 / 60000]\n",
      "loss: 0.345447 [12864 / 60000]\n",
      "loss: 0.565511 [19264 / 60000]\n",
      "loss: 0.496384 [25664 / 60000]\n",
      "loss: 0.504081 [32064 / 60000]\n",
      "loss: 0.500095 [38464 / 60000]\n",
      "loss: 0.671835 [44864 / 60000]\n",
      "loss: 0.612018 [51264 / 60000]\n",
      "loss: 0.468141 [57664 / 60000]\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.001913\n",
      "\n",
      "Epoch 9\n",
      "-------------\n",
      "loss: 0.403226 [   64 / 60000]\n",
      "loss: 0.529635 [ 6464 / 60000]\n",
      "loss: 0.342554 [12864 / 60000]\n",
      "loss: 0.561260 [19264 / 60000]\n",
      "loss: 0.492190 [25664 / 60000]\n",
      "loss: 0.500419 [32064 / 60000]\n",
      "loss: 0.496906 [38464 / 60000]\n",
      "loss: 0.671460 [44864 / 60000]\n",
      "loss: 0.610092 [51264 / 60000]\n",
      "loss: 0.463517 [57664 / 60000]\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.001899\n",
      "\n",
      "Epoch 10\n",
      "-------------\n",
      "loss: 0.398440 [   64 / 60000]\n",
      "loss: 0.526384 [ 6464 / 60000]\n",
      "loss: 0.339770 [12864 / 60000]\n",
      "loss: 0.557124 [19264 / 60000]\n",
      "loss: 0.488103 [25664 / 60000]\n",
      "loss: 0.496879 [32064 / 60000]\n",
      "loss: 0.493859 [38464 / 60000]\n",
      "loss: 0.670935 [44864 / 60000]\n",
      "loss: 0.608181 [51264 / 60000]\n",
      "loss: 0.459193 [57664 / 60000]\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.001886\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "\tprint(f\"Epoch {t+1}\\n-------------\")\n",
    "\ttrain_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "\ttest_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (saint)",
   "language": "python",
   "name": "saint"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
